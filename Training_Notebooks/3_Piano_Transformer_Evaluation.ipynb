{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎹 Piano Perception Transformer - Comprehensive Evaluation\n",
    "\n",
    "**Phase 3: Performance Analysis and Baseline Comparisons**\n",
    "\n",
    "This notebook implements comprehensive evaluation of the fine-tuned Piano Perception Transformer, including performance analysis, baseline comparisons, and model interpretability.\n",
    "\n",
    "**Pipeline Overview:**\n",
    "1. 🔧 **Setup & Environment** - Dependencies, model loading, evaluation framework\n",
    "2. 📊 **Model Performance Analysis** - Correlation, MSE, per-dimension analysis\n",
    "3. 🏆 **Baseline Comparisons** - CNN, Random Forest, Linear Regression baselines\n",
    "4. 🔍 **Model Interpretability** - Attention visualization, feature analysis\n",
    "5. 📈 **Results Visualization** - Comprehensive performance plots and analysis\n",
    "6. 📝 **Final Report** - Summary of findings and conclusions\n",
    "\n",
    "**Input:** Fine-tuned AST model from Phase 2  \n",
    "**Output:** Comprehensive evaluation report with performance metrics and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🔧 Cell 1: Setup and Model Loading\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"🚀 Setting up Piano Perception Transformer - Evaluation Phase...\")\n\n# Clone repo (skip if already exists)\nimport os\nif not os.path.exists('piano-perception-transformer'):\n    !git clone https://github.com/Jai-Dhiman/piano-perception-transformer.git\nelse:\n    print(\"Repository already exists, skipping clone...\")\n\n%cd piano-perception-transformer\n\n# Install uv\n!curl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Install enhanced dependencies including ML research tools\nprint(\"📦 Installing enhanced dependencies with uv...\")\n!export PATH=\"/usr/local/bin:$PATH\" && uv pip install --system jax[tpu] flax optax librosa pandas wandb requests zipfile36 scikit-learn scipy seaborn matplotlib pretty_midi soundfile plotly kaleido\n\n# Import core libraries\nimport sys\nimport json\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import cross_val_score\nimport jax\nimport jax.numpy as jnp\nfrom datetime import datetime\nfrom flax import linen as nn\nimport time\n\n# Initialize WandB for evaluation tracking\nimport wandb\n\ntry:\n    wandb.login()  # This will prompt for API key in Colab\n    \n    run = wandb.init(\n        project=\"piano-perception-transformer-evaluation\",\n        name=f\"ast-evaluation-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n        config={\n            \"phase\": \"comprehensive_evaluation\",\n            \"evaluation_type\": \"final_performance_analysis\",\n            \"metrics\": [\"correlation\", \"mse\", \"mae\", \"r2\", \"per_dimension_analysis\"],\n            \"baselines\": [\"random_forest\", \"linear_regression\", \"ridge_regression\", \"cnn_baseline\"],\n            \"interpretability\": [\"attention_maps\", \"feature_importance\", \"error_analysis\"],\n            \"target_correlation\": 0.7\n        },\n        tags=[\"evaluation\", \"ast\", \"percepiano\", \"analysis\", \"baselines\"]\n    )\n    \n    print(\"✅ WandB initialized successfully!\")\n    print(f\"   • Project: piano-perception-transformer-evaluation\")\n    print(f\"   • Run name: {run.name}\")\n    print(f\"   • Tracking: https://wandb.ai/{run.entity}/{run.project}/runs/{run.id}\")\n    \nexcept Exception as e:\n    print(f\"⚠️ WandB initialization failed: {e}\")\n    print(\"   • Continuing without experiment tracking\")\n\n# Mount Google Drive\nfrom google.colab import drive\nprint(\"🔗 Mounting Google Drive...\")\ndrive.mount('/content/drive')\n\n# Create directory structure\nbase_dir = '/content/drive/MyDrive/piano_transformer'\ndirectories = [\n    f'{base_dir}/processed_spectrograms',\n    f'{base_dir}/checkpoints/evaluation',\n    f'{base_dir}/logs',\n    f'{base_dir}/temp'\n]\n\nprint(\"📁 Setting up directory structure...\")\nfor directory in directories:\n    os.makedirs(directory, exist_ok=True)\n    print(f\"✅ Created: {directory}\")\n\n# Set up plotting style\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\n\nprint(f\"\\n🧠 JAX Configuration:\")\nprint(f\"   • Backend: {jax.default_backend()}\")\nprint(f\"   • Devices: {jax.device_count()}\")\nprint(f\"   • Device type: {jax.devices()[0].device_kind}\")\n\nprint(\"\\n✅ Evaluation setup completed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📊 Cell 2: Load Fine-tuned Model and Test Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "sys.path.append('/content/piano-perception-transformer/src')\n",
    "\n",
    "print(\"📂 Loading Fine-tuned Model and Test Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Define the model architecture (same as fine-tuning)\n",
    "class ProductionASTForRegression(nn.Module):\n",
    "    \"\"\"AST model with regression head for perceptual prediction\"\"\"\n",
    "    \n",
    "    patch_size: int = 16\n",
    "    embed_dim: int = 768\n",
    "    num_layers: int = 12\n",
    "    num_heads: int = 12\n",
    "    mlp_ratio: float = 4.0\n",
    "    dropout_rate: float = 0.1\n",
    "    attention_dropout: float = 0.1\n",
    "    stochastic_depth_rate: float = 0.1\n",
    "    num_outputs: int = 19\n",
    "    \n",
    "    def setup(self):\n",
    "        self.drop_rates = [\n",
    "            self.stochastic_depth_rate * i / (self.num_layers - 1) \n",
    "            for i in range(self.num_layers)\n",
    "        ]\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, training: bool = True):\n",
    "        batch_size, time_frames, freq_bins = x.shape\n",
    "        \n",
    "        # === PATCH EMBEDDING ===\n",
    "        patch_size = self.patch_size\n",
    "        \n",
    "        time_pad = (patch_size - time_frames % patch_size) % patch_size\n",
    "        freq_pad = (patch_size - freq_bins % patch_size) % patch_size\n",
    "        \n",
    "        if time_pad > 0 or freq_pad > 0:\n",
    "            x = jnp.pad(x, ((0, 0), (0, time_pad), (0, freq_pad)), mode='constant', constant_values=-80.0)\n",
    "        \n",
    "        time_patches = x.shape[1] // patch_size\n",
    "        freq_patches = x.shape[2] // patch_size\n",
    "        num_patches = time_patches * freq_patches\n",
    "        \n",
    "        x = x.reshape(batch_size, time_patches, patch_size, freq_patches, patch_size)\n",
    "        x = x.transpose(0, 1, 3, 2, 4)\n",
    "        x = x.reshape(batch_size, num_patches, patch_size * patch_size)\n",
    "        \n",
    "        x = nn.Dense(\n",
    "            self.embed_dim, \n",
    "            kernel_init=nn.initializers.truncated_normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            name='patch_embedding'\n",
    "        )(x)\n",
    "        \n",
    "        # === POSITIONAL ENCODING ===\n",
    "        pos_embedding = self.param(\n",
    "            'pos_embedding',\n",
    "            nn.initializers.truncated_normal(stddev=0.02),\n",
    "            (1, num_patches, self.embed_dim)\n",
    "        )\n",
    "        x = x + pos_embedding\n",
    "        \n",
    "        x = nn.Dropout(self.dropout_rate, deterministic=not training)(x)\n",
    "        \n",
    "        # === TRANSFORMER LAYERS ===\n",
    "        for layer_idx in range(self.num_layers):\n",
    "            drop_rate = self.drop_rates[layer_idx]\n",
    "            \n",
    "            # Self-Attention\n",
    "            residual = x\n",
    "            x = nn.LayerNorm(epsilon=1e-6, name=f'norm1_layer{layer_idx}')(x)\n",
    "            \n",
    "            attention = nn.MultiHeadDotProductAttention(\n",
    "                num_heads=self.num_heads,\n",
    "                dropout_rate=self.attention_dropout,\n",
    "                kernel_init=nn.initializers.truncated_normal(stddev=0.02),\n",
    "                bias_init=nn.initializers.zeros,\n",
    "                name=f'attention_layer{layer_idx}'\n",
    "            )(x, x, deterministic=not training)\n",
    "            \n",
    "            # Stochastic depth for attention\n",
    "            if training and drop_rate > 0:\n",
    "                random_tensor = jax.random.uniform(\n",
    "                    self.make_rng('stochastic_depth'), (batch_size, 1, 1)\n",
    "                )\n",
    "                keep_prob = 1.0 - drop_rate\n",
    "                binary_tensor = (random_tensor < keep_prob).astype(x.dtype)\n",
    "                attention = attention * binary_tensor / keep_prob\n",
    "            \n",
    "            x = residual + nn.Dropout(self.dropout_rate, deterministic=not training)(attention)\n",
    "            \n",
    "            # MLP\n",
    "            residual = x\n",
    "            x = nn.LayerNorm(epsilon=1e-6, name=f'norm2_layer{layer_idx}')(x)\n",
    "            \n",
    "            mlp_hidden = int(self.embed_dim * self.mlp_ratio)\n",
    "            \n",
    "            mlp = nn.Dense(\n",
    "                mlp_hidden, \n",
    "                kernel_init=nn.initializers.truncated_normal(stddev=0.02),\n",
    "                bias_init=nn.initializers.zeros,\n",
    "                name=f'mlp_dense1_layer{layer_idx}'\n",
    "            )(x)\n",
    "            mlp = nn.gelu(mlp)\n",
    "            mlp = nn.Dropout(self.dropout_rate, deterministic=not training)(mlp)\n",
    "            \n",
    "            mlp = nn.Dense(\n",
    "                self.embed_dim,\n",
    "                kernel_init=nn.initializers.truncated_normal(stddev=0.02),\n",
    "                bias_init=nn.initializers.zeros,\n",
    "                name=f'mlp_dense2_layer{layer_idx}'\n",
    "            )(mlp)\n",
    "            \n",
    "            # Stochastic depth for MLP\n",
    "            if training and drop_rate > 0:\n",
    "                random_tensor = jax.random.uniform(\n",
    "                    self.make_rng('stochastic_depth'), (batch_size, 1, 1)\n",
    "                )\n",
    "                keep_prob = 1.0 - drop_rate\n",
    "                binary_tensor = (random_tensor < keep_prob).astype(x.dtype)\n",
    "                mlp = mlp * binary_tensor / keep_prob\n",
    "            \n",
    "            x = residual + nn.Dropout(self.dropout_rate, deterministic=not training)(mlp)\n",
    "        \n",
    "        # === FINAL PROCESSING ===\n",
    "        x = nn.LayerNorm(epsilon=1e-6, name='final_norm')(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = jnp.mean(x, axis=1)  # [batch, embed_dim]\n",
    "        \n",
    "        # === REGRESSION HEAD ===\n",
    "        x = nn.Dense(\n",
    "            512, \n",
    "            kernel_init=nn.initializers.truncated_normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            name='regression_hidden'\n",
    "        )(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dropout(self.dropout_rate, deterministic=not training)(x)\n",
    "        \n",
    "        predictions = nn.Dense(\n",
    "            self.num_outputs,\n",
    "            kernel_init=nn.initializers.truncated_normal(stddev=0.02),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            name='regression_output'\n",
    "        )(x)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "def load_finetuned_model(checkpoint_path):\n",
    "    \"\"\"Load fine-tuned model checkpoint\"\"\"\n",
    "    print(f\"📂 Loading fine-tuned model: {checkpoint_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(checkpoint_path, 'rb') as f:\n",
    "            checkpoint = pickle.load(f)\n",
    "        \n",
    "        print(f\"✅ Model checkpoint loaded successfully\")\n",
    "        \n",
    "        # Handle potential string/None values for correlation\n",
    "        best_val_corr = checkpoint.get('best_val_correlation', 'N/A')\n",
    "        if isinstance(best_val_corr, (int, float)) and best_val_corr is not None:\n",
    "            print(f\"   • Best validation correlation: {best_val_corr:.4f}\")\n",
    "        else:\n",
    "            print(f\"   • Best validation correlation: {best_val_corr}\")\n",
    "            \n",
    "        print(f\"   • Training epochs: {checkpoint.get('finetuning_results', {}).get('total_epochs', 'N/A')}\")\n",
    "        print(f\"   • Model parameters: {sum(x.size for x in jax.tree.leaves(checkpoint['params'])):,}\")\n",
    "        \n",
    "        return checkpoint\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model checkpoint: {e}\")\n",
    "        raise\n",
    "\n",
    "def model_predict(model, params, batch_specs):\n",
    "    \"\"\"Model prediction function (not JIT-compiled to avoid method issues)\"\"\"\n",
    "    return model.apply(params, batch_specs, training=False)\n",
    "\n",
    "def evaluate_model_on_dataset(model, params, dataset, batch_size=32, description=\"Dataset\"):\n",
    "    \"\"\"Evaluate model on a dataset and return predictions and targets\"\"\"\n",
    "    print(f\"🔍 Evaluating model on {description}...\")\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    num_samples = len(dataset)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        # Get batch\n",
    "        batch_specs, batch_targets = dataset.get_batch(batch_size, shuffle=False)\n",
    "        batch_specs = jnp.array(batch_specs)\n",
    "        \n",
    "        # Model prediction\n",
    "        predictions = model_predict(model, params, batch_specs)\n",
    "        \n",
    "        # Convert to numpy and store\n",
    "        all_predictions.append(np.array(predictions))\n",
    "        all_targets.append(np.array(batch_targets))\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"   Processed {batch_idx + 1}/{num_batches} batches...\")\n",
    "    \n",
    "    # Concatenate all results\n",
    "    predictions = np.concatenate(all_predictions, axis=0)[:num_samples]  # Trim to exact size\n",
    "    targets = np.concatenate(all_targets, axis=0)[:num_samples]\n",
    "    \n",
    "    print(f\"✅ Evaluation completed on {predictions.shape[0]} samples\")\n",
    "    print(f\"   • Predictions shape: {predictions.shape}\")\n",
    "    print(f\"   • Targets shape: {targets.shape}\")\n",
    "    \n",
    "    return predictions, targets\n",
    "\n",
    "# Load fine-tuned model\n",
    "finetuned_checkpoint_path = '/content/drive/MyDrive/piano_transformer/checkpoints/finetuning/final_finetuned_model.pkl'\n",
    "\n",
    "if os.path.exists(finetuned_checkpoint_path):\n",
    "    finetuned_checkpoint = load_finetuned_model(finetuned_checkpoint_path)\n",
    "    model_params = finetuned_checkpoint['params']\n",
    "    label_scaler = finetuned_checkpoint['label_scaler']\n",
    "    print(f\"✅ Fine-tuned model loaded successfully!\")\n",
    "else:\n",
    "    print(f\"❌ Fine-tuned model not found: {finetuned_checkpoint_path}\")\n",
    "    print(f\"   Please run the fine-tuning notebook first\")\n",
    "    raise FileNotFoundError(\"Fine-tuned model checkpoint not found\")\n",
    "\n",
    "# Initialize model\n",
    "eval_model = ProductionASTForRegression(\n",
    "    patch_size=16,\n",
    "    embed_dim=768,\n",
    "    num_layers=12,\n",
    "    num_heads=12,\n",
    "    mlp_ratio=4.0,\n",
    "    dropout_rate=0.1,\n",
    "    attention_dropout=0.1,\n",
    "    stochastic_depth_rate=0.1,\n",
    "    num_outputs=19\n",
    ")\n",
    "\n",
    "print(f\"\\n🧠 Model Architecture Summary:\")\n",
    "print(f\"   • Total parameters: {sum(x.size for x in jax.tree.leaves(model_params)):,}\")\n",
    "print(f\"   • Architecture: 12-layer AST + regression head\")\n",
    "print(f\"   • Input: 128×128 mel-spectrograms\")\n",
    "print(f\"   • Output: 19 perceptual dimensions\")\n",
    "\n",
    "# Note: We'll load the test dataset in the next cell when we evaluate baselines\n",
    "print(f\"\\n🎯 Model loaded and ready for comprehensive evaluation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🏆 Cell 3: Load Test Data and Baseline Models\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone and setup PercePiano dataset\nprint(\"📂 Setting up PercePiano dataset for evaluation...\")\n\n# Define PercePiano directory path\npercepiano_dir = '/content/drive/MyDrive/PercePiano'\n\n# Clone PercePiano dataset if not exists\nif not os.path.exists(percepiano_dir):\n    print(\"📥 Cloning PercePiano dataset repository...\")\n    !git clone https://github.com/JonghoKimSNU/PercePiano.git {percepiano_dir}\n    print(\"✅ PercePiano dataset cloned successfully!\")\nelse:\n    print(\"✅ PercePiano dataset already exists\")\n\n# Verify essential directory structure\nrequired_paths = [\n    f'{percepiano_dir}/labels/label_2round_mean_reg_19_with0_rm_highstd0.json',\n    f'{percepiano_dir}/virtuoso/data/all_2rounds'\n]\n\nprint(\"🔍 Verifying dataset structure...\")\nmissing_paths = []\nfor path in required_paths:\n    if not os.path.exists(path):\n        missing_paths.append(path)\n    else:\n        if path.endswith('.json'):\n            print(f\"   ✅ Labels file found: {os.path.basename(path)}\")\n        else:\n            midi_count = len([f for f in os.listdir(path) if f.endswith('.mid')])\n            print(f\"   ✅ MIDI directory found: {midi_count} MIDI files\")\n\nif missing_paths:\n    print(\"❌ Missing required files/directories:\")\n    for path in missing_paths:\n        print(f\"   • {path}\")\n    print(\"\\n💡 The dataset may need to be downloaded separately.\")\n    print(\"   Please refer to the PercePiano repository instructions.\")\n    raise FileNotFoundError(\"PercePiano dataset structure incomplete\")\n\nprint(\"✅ Dataset structure verified successfully!\")\n\n# Import required libraries for baselines and data loading\nimport json\nimport pretty_midi\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr\n\nprint(\"\\n📂 Loading Test Data and Creating Baseline Models\")\nprint(\"=\"*60)\n\ndef midi_to_spectrogram(midi_path, sr=22050, n_mels=128, hop_length=512, n_fft=2048, target_length=128):\n    \"\"\"Convert MIDI to mel-spectrogram\"\"\"\n    try:\n        midi_data = pretty_midi.PrettyMIDI(midi_path)\n        try:\n            audio = midi_data.fluidsynth(fs=sr)\n        except:\n            audio = midi_data.synthesize(fs=sr)\n        \n        # Ensure minimum audio length\n        min_duration = 2.0\n        min_samples = int(min_duration * sr)\n        if len(audio) < min_samples:\n            padding = min_samples - len(audio)\n            audio = np.pad(audio, (0, padding), mode='constant')\n        \n        # Convert to mel-spectrogram\n        mel_spec = librosa.feature.melspectrogram(\n            y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, n_fft=n_fft,\n            power=2.0, fmin=20, fmax=sr//2\n        )\n        \n        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n        mel_spec_transposed = mel_spec_db.T\n        \n        current_length = mel_spec_transposed.shape[0]\n        if current_length >= target_length:\n            normalized_spec = mel_spec_transposed[:target_length, :]\n        else:\n            pad_width = target_length - current_length\n            normalized_spec = np.pad(\n                mel_spec_transposed, ((0, pad_width), (0, 0)), \n                mode='constant', constant_values=-80.0\n            )\n        \n        return normalized_spec\n    except Exception as e:\n        print(f\"Error converting MIDI {midi_path}: {str(e)}\")\n        return None\n\ndef load_percepiano_data(percepiano_dir):\n    \"\"\"Load PercePiano dataset\"\"\"\n    labels_file = f'{percepiano_dir}/labels/label_2round_mean_reg_19_with0_rm_highstd0.json'\n    \n    print(f\"📋 Loading labels from: {labels_file}\")\n    with open(labels_file, 'r') as f:\n        labels_data = json.load(f)\n\n    print(f\"📊 Loaded PercePiano labels: {len(labels_data)} samples\")\n\n    midi_dir = f'{percepiano_dir}/virtuoso/data/all_2rounds'\n    \n    if not os.path.exists(midi_dir):\n        raise FileNotFoundError(f\"MIDI directory not found: {midi_dir}\")\n    \n    midi_files = [f for f in os.listdir(midi_dir) if f.endswith('.mid')]\n\n    if len(midi_files) == 0:\n        raise FileNotFoundError(f\"No MIDI files found in: {midi_dir}\")\n\n    print(f\"🎵 Found {len(midi_files)} MIDI files\")\n\n    samples = []\n    processed_count = 0\n\n    for filename, label_data in labels_data.items():\n        # Find corresponding MIDI file (flexible matching)\n        midi_filename = None\n        for midi_file in midi_files:\n            # Try multiple matching strategies\n            if (filename in midi_file or \n                midi_file.replace('.mid', '') in filename or\n                filename.replace('.mid', '') in midi_file.replace('.mid', '')):\n                midi_filename = midi_file\n                break\n\n        if midi_filename is None:\n            continue\n\n        # Extract the 19 perceptual features\n        if isinstance(label_data, list) and len(label_data) >= 19:\n            perceptual_features = np.array(label_data[:19], dtype=np.float32)\n        else:\n            continue\n\n        # Convert MIDI to spectrogram\n        midi_path = os.path.join(midi_dir, midi_filename)\n        spectrogram = midi_to_spectrogram(midi_path, target_length=128)\n        \n        if spectrogram is not None and spectrogram.shape == (128, 128):\n            samples.append({\n                'spectrogram': spectrogram,\n                'labels': perceptual_features,\n                'filename': filename\n            })\n            processed_count += 1\n            \n            if processed_count % 25 == 0:\n                print(f\"📊 Processed {processed_count} samples...\")\n\n    print(f\"✅ Successfully processed {processed_count} samples\")\n    return samples\n\nclass PercePianoDataset:\n    \"\"\"PercePiano dataset class\"\"\"\n    def __init__(self, samples, split='train', train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_seed=42):\n        self.split = split\n        self.random_seed = random_seed\n        \n        np.random.seed(random_seed)\n        \n        spectrograms = [s['spectrogram'] for s in samples]\n        labels = [s['labels'] for s in samples]\n        filenames = [s['filename'] for s in samples]\n        \n        # Create splits\n        train_specs, temp_specs, train_labels, temp_labels, train_files, temp_files = train_test_split(\n            spectrograms, labels, filenames,\n            test_size=(val_ratio + test_ratio), \n            random_state=random_seed\n        )\n        \n        val_size = val_ratio / (val_ratio + test_ratio)\n        val_specs, test_specs, val_labels, test_labels, val_files, test_files = train_test_split(\n            temp_specs, temp_labels, temp_files,\n            test_size=(1 - val_size), \n            random_state=random_seed\n        )\n        \n        # Assign data based on split\n        if split == 'train':\n            self.spectrograms = np.array(train_specs)\n            self.labels = np.array(train_labels)\n            self.filenames = train_files\n        elif split == 'val':\n            self.spectrograms = np.array(val_specs)\n            self.labels = np.array(val_labels)\n            self.filenames = val_files\n        elif split == 'test':\n            self.spectrograms = np.array(test_specs)\n            self.labels = np.array(test_labels)\n            self.filenames = test_files\n        \n        self.num_samples = len(self.spectrograms)\n        \n        print(f\"📊 {split.title()} split: {self.num_samples} samples\")\n    \n    def set_label_scaler(self, scaler):\n        \"\"\"Apply label normalization\"\"\"\n        self.labels = scaler.transform(self.labels)\n        print(f\"✅ Applied label normalization to {self.split} split\")\n    \n    def __len__(self):\n        return self.num_samples\n    \n    def get_batch(self, batch_size, shuffle=False):\n        \"\"\"Get a batch of data\"\"\"\n        if shuffle:\n            indices = np.random.choice(self.num_samples, size=batch_size, replace=True)\n        else:\n            start_idx = np.random.randint(0, max(1, self.num_samples - batch_size + 1))\n            indices = np.arange(start_idx, start_idx + batch_size) % self.num_samples\n        \n        batch_specs = self.spectrograms[indices]\n        batch_labels = self.labels[indices]\n        \n        return batch_specs, batch_labels\n    \n    def get_all_data(self):\n        \"\"\"Get all data at once\"\"\"\n        return self.spectrograms, self.labels\n\n# Load PercePiano dataset for evaluation\nprint(\"\\n🔄 Loading and processing PercePiano dataset for evaluation...\")\n\ntry:\n    # Load data\n    raw_samples = load_percepiano_data(percepiano_dir)\n\n    # Create datasets\n    train_dataset = PercePianoDataset(raw_samples, split='train', random_seed=42)\n    val_dataset = PercePianoDataset(raw_samples, split='val', random_seed=42)\n    test_dataset = PercePianoDataset(raw_samples, split='test', random_seed=42)\n\n    # Apply same label scaling as used in training\n    train_dataset.labels = label_scaler.fit_transform(train_dataset.labels)  # Refit for consistency\n    val_dataset.set_label_scaler(label_scaler)\n    test_dataset.set_label_scaler(label_scaler)\n\n    print(f\"\\n✅ Datasets loaded successfully!\")\n    print(f\"   • Training dataset: {len(train_dataset)} samples\")\n    print(f\"   • Validation dataset: {len(val_dataset)} samples\")\n    print(f\"   • Test dataset: {len(test_dataset)} samples\")\n\n    # Prepare baseline features (flattened spectrograms + statistical features)\n    def extract_baseline_features(spectrograms):\n        \"\"\"Extract features for baseline models\"\"\"\n        features = []\n        \n        for spec in spectrograms:\n            # Flatten spectrogram\n            flat_spec = spec.flatten()\n            \n            # Statistical features\n            stats = [\n                np.mean(spec), np.std(spec), np.min(spec), np.max(spec),\n                np.median(spec), np.percentile(spec, 25), np.percentile(spec, 75),\n                np.mean(spec, axis=0).mean(), np.std(spec, axis=0).mean(),  # Frequency stats\n                np.mean(spec, axis=1).mean(), np.std(spec, axis=1).mean(),  # Time stats\n            ]\n            \n            # Combine flattened spec and stats (subsample for memory efficiency)\n            subsampled_spec = flat_spec[::4]  # Take every 4th element\n            combined = np.concatenate([subsampled_spec, stats])\n            features.append(combined)\n        \n        return np.array(features)\n\n    print(\"\\n🔧 Extracting baseline features...\")\n    train_specs, train_labels = train_dataset.get_all_data()\n    val_specs, val_labels = val_dataset.get_all_data()\n    test_specs, test_labels = test_dataset.get_all_data()\n\n    # Extract features for baselines\n    train_features = extract_baseline_features(train_specs)\n    val_features = extract_baseline_features(val_specs)\n    test_features = extract_baseline_features(test_specs)\n\n    print(f\"✅ Baseline features extracted\")\n    print(f\"   • Feature dimensions: {train_features.shape[1]}\")\n    print(f\"   • Train features: {train_features.shape}\")\n    print(f\"   • Val features: {val_features.shape}\")\n    print(f\"   • Test features: {test_features.shape}\")\n\n    # Train baseline models\n    print(\"\\n🏗️ Training baseline models...\")\n\n    baselines = {\n        'Linear Regression': LinearRegression(),\n        'Ridge Regression': Ridge(alpha=1.0),\n        'Random Forest': RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n        'MLP': MLPRegressor(hidden_layer_sizes=(512, 256), max_iter=300, random_state=42)\n    }\n\n    trained_baselines = {}\n\n    for name, model in baselines.items():\n        print(f\"   Training {name}...\")\n        start_time = time.time()\n        \n        try:\n            model.fit(train_features, train_labels)\n            trained_baselines[name] = model\n            \n            training_time = time.time() - start_time\n            print(f\"     ✅ {name} trained in {training_time:.1f}s\")\n        except Exception as e:\n            print(f\"     ❌ {name} training failed: {e}\")\n\n    print(f\"\\n✅ Baseline models trained successfully!\")\n    print(f\"   • {len(trained_baselines)} baseline models ready\")\n    print(f\"\\n🎯 Ready for comprehensive evaluation!\")\n\nexcept Exception as e:\n    print(f\"❌ PercePiano data loading failed: {e}\")\n    print(f\"   Error details: {str(e)}\")\n    print(f\"\\n💡 Troubleshooting tips:\")\n    print(f\"   1. Check if PercePiano repository was cloned correctly\")\n    print(f\"   2. Verify internet connection for cloning\")\n    print(f\"   3. Ensure sufficient disk space in Google Drive\")\n    raise Exception(f\"PercePiano dataset setup failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📈 Cell 4: Comprehensive Model Evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📈 Comprehensive Model Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def compute_detailed_metrics(predictions, targets, model_name=\"Model\"):\n",
    "    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Overall metrics\n",
    "    mse = mean_squared_error(targets, predictions)\n",
    "    mae = mean_absolute_error(targets, predictions)\n",
    "    \n",
    "    # Overall correlation\n",
    "    flat_pred = predictions.flatten()\n",
    "    flat_target = targets.flatten()\n",
    "    pearson_corr, _ = pearsonr(flat_pred, flat_target)\n",
    "    \n",
    "    # R² score\n",
    "    r2 = r2_score(targets, predictions)\n",
    "    \n",
    "    metrics.update({\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'rmse': np.sqrt(mse),\n",
    "        'pearson_correlation': pearson_corr,\n",
    "        'r2_score': r2\n",
    "    })\n",
    "    \n",
    "    # Per-dimension metrics\n",
    "    per_dim_corr = []\n",
    "    per_dim_mse = []\n",
    "    per_dim_mae = []\n",
    "    \n",
    "    for dim in range(predictions.shape[1]):\n",
    "        dim_pred = predictions[:, dim]\n",
    "        dim_target = targets[:, dim]\n",
    "        \n",
    "        if np.std(dim_pred) > 1e-8 and np.std(dim_target) > 1e-8:\n",
    "            corr, _ = pearsonr(dim_pred, dim_target)\n",
    "            per_dim_corr.append(corr)\n",
    "        else:\n",
    "            per_dim_corr.append(0.0)\n",
    "        \n",
    "        per_dim_mse.append(mean_squared_error(dim_target, dim_pred))\n",
    "        per_dim_mae.append(mean_absolute_error(dim_target, dim_pred))\n",
    "    \n",
    "    metrics.update({\n",
    "        'per_dimension_correlation': per_dim_corr,\n",
    "        'per_dimension_mse': per_dim_mse,\n",
    "        'per_dimension_mae': per_dim_mae,\n",
    "        'mean_per_dim_correlation': np.mean(per_dim_corr),\n",
    "        'std_per_dim_correlation': np.std(per_dim_corr)\n",
    "    })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, model_name):\n",
    "    \"\"\"Print evaluation metrics in a formatted way\"\"\"\n",
    "    print(f\"\\n📊 {model_name} Results:\")\n",
    "    print(f\"   • Pearson Correlation: {metrics['pearson_correlation']:.4f}\")\n",
    "    print(f\"   • Mean Per-Dim Correlation: {metrics['mean_per_dim_correlation']:.4f} ± {metrics['std_per_dim_correlation']:.4f}\")\n",
    "    print(f\"   • RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"   • MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"   • R² Score: {metrics['r2_score']:.4f}\")\n",
    "\n",
    "# Evaluate Piano Transformer (our model)\n",
    "print(\"🎹 Evaluating Piano Perception Transformer...\")\n",
    "\n",
    "# Get predictions on test set\n",
    "ast_test_predictions, ast_test_targets = evaluate_model_on_dataset(\n",
    "    eval_model, model_params, test_dataset, batch_size=32, description=\"Test Set\"\n",
    ")\n",
    "\n",
    "# Compute metrics for AST\n",
    "ast_metrics = compute_detailed_metrics(ast_test_predictions, ast_test_targets, \"Piano Transformer\")\n",
    "print_metrics(ast_metrics, \"🎹 Piano Perception Transformer\")\n",
    "\n",
    "# Evaluate baseline models\n",
    "print(\"\\n🏆 Evaluating Baseline Models...\")\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "for name, model in trained_baselines.items():\n",
    "    print(f\"\\n   Evaluating {name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Get predictions\n",
    "        baseline_predictions = model.predict(test_features)\n",
    "        \n",
    "        # Compute metrics\n",
    "        baseline_metrics = compute_detailed_metrics(baseline_predictions, test_labels, name)\n",
    "        baseline_results[name] = {\n",
    "            'predictions': baseline_predictions,\n",
    "            'metrics': baseline_metrics\n",
    "        }\n",
    "        \n",
    "        print_metrics(baseline_metrics, f\"🏆 {name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"     ❌ {name} evaluation failed: {e}\")\n",
    "\n",
    "# Create comparison table\n",
    "print(\"\\n📋 COMPREHENSIVE COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare comparison data\n",
    "comparison_data = []\n",
    "\n",
    "# Add AST results\n",
    "comparison_data.append({\n",
    "    'Model': '🎹 Piano Transformer (AST)',\n",
    "    'Pearson Corr': ast_metrics['pearson_correlation'],\n",
    "    'Mean Per-Dim Corr': ast_metrics['mean_per_dim_correlation'],\n",
    "    'RMSE': ast_metrics['rmse'],\n",
    "    'MAE': ast_metrics['mae'],\n",
    "    'R² Score': ast_metrics['r2_score']\n",
    "})\n",
    "\n",
    "# Add baseline results\n",
    "for name, result in baseline_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': f'🏆 {name}',\n",
    "        'Pearson Corr': result['metrics']['pearson_correlation'],\n",
    "        'Mean Per-Dim Corr': result['metrics']['mean_per_dim_correlation'],\n",
    "        'RMSE': result['metrics']['rmse'],\n",
    "        'MAE': result['metrics']['mae'],\n",
    "        'R² Score': result['metrics']['r2_score']\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Pearson Corr', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Best model analysis\n",
    "best_model = comparison_df.iloc[0]\n",
    "print(f\"\\n🏆 BEST PERFORMING MODEL: {best_model['Model']}\")\n",
    "print(f\"   • Pearson Correlation: {best_model['Pearson Corr']:.4f}\")\n",
    "print(f\"   • Mean Per-Dimension Correlation: {best_model['Mean Per-Dim Corr']:.4f}\")\n",
    "\n",
    "# Performance analysis\n",
    "ast_rank = comparison_df[comparison_df['Model'].str.contains('Piano Transformer')].index[0] + 1\n",
    "total_models = len(comparison_df)\n",
    "\n",
    "print(f\"\\n📈 PIANO TRANSFORMER PERFORMANCE ANALYSIS:\")\n",
    "print(f\"   • Rank: {ast_rank}/{total_models}\")\n",
    "print(f\"   • Performance vs Best: {ast_metrics['pearson_correlation']/best_model['Pearson Corr']*100:.1f}%\")\n",
    "\n",
    "if ast_rank == 1:\n",
    "    print(f\"   🎉 OUTSTANDING! Piano Transformer achieves BEST performance\")\n",
    "elif ast_rank <= 2:\n",
    "    print(f\"   ✅ EXCELLENT! Piano Transformer in top 2 performers\")\n",
    "elif ast_rank <= len(comparison_df) // 2:\n",
    "    print(f\"   ⚠️ MODERATE performance - room for improvement\")\n",
    "else:\n",
    "    print(f\"   ❌ BELOW AVERAGE performance - significant improvement needed\")\n",
    "\n",
    "# Log results to WandB\n",
    "try:\n",
    "    wandb.log({\n",
    "        \"ast_pearson_correlation\": ast_metrics['pearson_correlation'],\n",
    "        \"ast_mean_per_dim_correlation\": ast_metrics['mean_per_dim_correlation'],\n",
    "        \"ast_rmse\": ast_metrics['rmse'],\n",
    "        \"ast_mae\": ast_metrics['mae'],\n",
    "        \"ast_r2_score\": ast_metrics['r2_score'],\n",
    "        \"ast_rank\": ast_rank,\n",
    "        \"total_models_compared\": total_models,\n",
    "        \"best_model_correlation\": best_model['Pearson Corr']\n",
    "    })\n",
    "    \n",
    "    # Log per-dimension correlations\n",
    "    for i, corr in enumerate(ast_metrics['per_dimension_correlation']):\n",
    "        wandb.log({f\"ast_correlation_dim_{i+1}\": corr})\n",
    "    \n",
    "    print(f\"\\n✅ Results logged to WandB\")\n",
    "except:\n",
    "    print(f\"\\n⚠️ WandB logging failed - continuing without logging\")\n",
    "\n",
    "print(f\"\\n🎯 Comprehensive evaluation completed!\")\n",
    "\n",
    "# Store results for visualization\n",
    "evaluation_results = {\n",
    "    'ast_metrics': ast_metrics,\n",
    "    'ast_predictions': ast_test_predictions,\n",
    "    'ast_targets': ast_test_targets,\n",
    "    'baseline_results': baseline_results,\n",
    "    'test_features': test_features,\n",
    "    'test_labels': test_labels,\n",
    "    'comparison_df': comparison_df\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 Results stored for visualization in next cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📊 Cell 5: Results Visualization and Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📊 Creating Comprehensive Visualizations\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set up plotting\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Define perceptual dimension names (assuming standard PercePiano dimensions)\n",
    "DIMENSION_NAMES = [\n",
    "    'Articulation', 'Attack', 'Brightness', 'Depth', 'Dynamics',\n",
    "    'Fluidity', 'Pace', 'Pedaling', 'Precision', 'Rhythm',\n",
    "    'Rubato', 'Softness', 'Spacing', 'Stability', 'Strength',\n",
    "    'Tension', 'Texture', 'Timing', 'Touch'\n",
    "]\n",
    "\n",
    "# 1. Model Comparison Bar Chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Overall correlation comparison\n",
    "models = evaluation_results['comparison_df']['Model'].values\n",
    "correlations = evaluation_results['comparison_df']['Pearson Corr'].values\n",
    "\n",
    "colors = ['#FF6B6B' if 'Piano Transformer' in model else '#4ECDC4' for model in models]\n",
    "bars1 = ax1.bar(range(len(models)), correlations, color=colors, alpha=0.7)\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('Pearson Correlation')\n",
    "ax1.set_title('🏆 Model Performance Comparison - Overall Correlation')\n",
    "ax1.set_xticks(range(len(models)))\n",
    "ax1.set_xticklabels([m.replace('🎹 ', '').replace('🏆 ', '') for m in models], rotation=45, ha='right')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, corr in zip(bars1, correlations):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{corr:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Per-dimension correlation comparison\n",
    "per_dim_correlations = evaluation_results['comparison_df']['Mean Per-Dim Corr'].values\n",
    "bars2 = ax2.bar(range(len(models)), per_dim_correlations, color=colors, alpha=0.7)\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('Mean Per-Dimension Correlation')\n",
    "ax2.set_title('📊 Model Performance - Mean Per-Dimension Correlation')\n",
    "ax2.set_xticks(range(len(models)))\n",
    "ax2.set_xticklabels([m.replace('🎹 ', '').replace('🏆 ', '') for m in models], rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, corr in zip(bars2, per_dim_correlations):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{corr:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/piano_transformer/evaluation_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Per-Dimension Performance Analysis\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "ast_per_dim_corr = evaluation_results['ast_metrics']['per_dimension_correlation']\n",
    "x_pos = np.arange(len(DIMENSION_NAMES))\n",
    "\n",
    "# Create bars with color coding based on performance\n",
    "colors = ['#2ECC71' if corr >= 0.7 else '#F39C12' if corr >= 0.5 else '#E74C3C' for corr in ast_per_dim_corr]\n",
    "bars = ax.bar(x_pos, ast_per_dim_corr, color=colors, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Perceptual Dimensions')\n",
    "ax.set_ylabel('Pearson Correlation')\n",
    "ax.set_title('🎹 Piano Transformer: Per-Dimension Performance Analysis')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(DIMENSION_NAMES, rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add horizontal lines for performance thresholds\n",
    "ax.axhline(y=0.7, color='green', linestyle='--', alpha=0.7, label='Excellent (≥0.7)')\n",
    "ax.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Good (≥0.5)')\n",
    "ax.axhline(y=0.3, color='red', linestyle='--', alpha=0.7, label='Moderate (≥0.3)')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, corr in zip(bars, ast_per_dim_corr):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{corr:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/piano_transformer/evaluation_per_dimension.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Prediction vs. Target Scatter Plots (for best and worst dimensions)\n",
    "best_dim_idx = np.argmax(ast_per_dim_corr)\n",
    "worst_dim_idx = np.argmin(ast_per_dim_corr)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Best dimension\n",
    "best_predictions = evaluation_results['ast_predictions'][:, best_dim_idx]\n",
    "best_targets = evaluation_results['ast_targets'][:, best_dim_idx]\n",
    "ax1.scatter(best_targets, best_predictions, alpha=0.6, color='#2ECC71')\n",
    "ax1.plot([best_targets.min(), best_targets.max()], [best_targets.min(), best_targets.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('True Values')\n",
    "ax1.set_ylabel('Predicted Values')\n",
    "ax1.set_title(f'✅ Best Dimension: {DIMENSION_NAMES[best_dim_idx]}\\n(r = {ast_per_dim_corr[best_dim_idx]:.3f})')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Worst dimension\n",
    "worst_predictions = evaluation_results['ast_predictions'][:, worst_dim_idx]\n",
    "worst_targets = evaluation_results['ast_targets'][:, worst_dim_idx]\n",
    "ax2.scatter(worst_targets, worst_predictions, alpha=0.6, color='#E74C3C')\n",
    "ax2.plot([worst_targets.min(), worst_targets.max()], [worst_targets.min(), worst_targets.max()], 'r--', lw=2)\n",
    "ax2.set_xlabel('True Values')\n",
    "ax2.set_ylabel('Predicted Values')\n",
    "ax2.set_title(f'❌ Worst Dimension: {DIMENSION_NAMES[worst_dim_idx]}\\n(r = {ast_per_dim_corr[worst_dim_idx]:.3f})')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/piano_transformer/evaluation_scatter_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Error Analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Residuals distribution\n",
    "residuals = evaluation_results['ast_predictions'] - evaluation_results['ast_targets']\n",
    "ax1.hist(residuals.flatten(), bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_xlabel('Prediction Residuals')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('📊 Residuals Distribution')\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Per-dimension RMSE\n",
    "per_dim_rmse = np.sqrt(evaluation_results['ast_metrics']['per_dimension_mse'])\n",
    "bars = ax2.bar(range(len(DIMENSION_NAMES)), per_dim_rmse, color='lightcoral', alpha=0.7)\n",
    "ax2.set_xlabel('Perceptual Dimensions')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('📈 Per-Dimension Root Mean Squared Error')\n",
    "ax2.set_xticks(range(len(DIMENSION_NAMES)))\n",
    "ax2.set_xticklabels(DIMENSION_NAMES, rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/piano_transformer/evaluation_error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Performance Summary Heatmap\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create heatmap data: models vs metrics\n",
    "metrics_names = ['Pearson Corr', 'Mean Per-Dim Corr', 'R² Score']\n",
    "model_names = [m.replace('🎹 ', '').replace('🏆 ', '') for m in evaluation_results['comparison_df']['Model']]\n",
    "\n",
    "heatmap_data = []\n",
    "for _, row in evaluation_results['comparison_df'].iterrows():\n",
    "    heatmap_data.append([\n",
    "        row['Pearson Corr'],\n",
    "        row['Mean Per-Dim Corr'],\n",
    "        row['R² Score']\n",
    "    ])\n",
    "\n",
    "heatmap_data = np.array(heatmap_data)\n",
    "\n",
    "im = ax.imshow(heatmap_data, cmap='RdYlGn', aspect='auto')\n",
    "ax.set_xticks(range(len(metrics_names)))\n",
    "ax.set_xticklabels(metrics_names)\n",
    "ax.set_yticks(range(len(model_names)))\n",
    "ax.set_yticklabels(model_names)\n",
    "ax.set_title('🔥 Model Performance Heatmap')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(metrics_names)):\n",
    "        text = ax.text(j, i, f'{heatmap_data[i, j]:.3f}',\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.savefig('/content/drive/MyDrive/piano_transformer/evaluation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Performance Summary Statistics\n",
    "print(\"\\n📋 DETAILED PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n🎹 Piano Transformer Performance:\")\n",
    "print(f\"   • Overall Correlation: {evaluation_results['ast_metrics']['pearson_correlation']:.4f}\")\n",
    "print(f\"   • Best Dimension: {DIMENSION_NAMES[best_dim_idx]} (r = {ast_per_dim_corr[best_dim_idx]:.4f})\")\n",
    "print(f\"   • Worst Dimension: {DIMENSION_NAMES[worst_dim_idx]} (r = {ast_per_dim_corr[worst_dim_idx]:.4f})\")\n",
    "\n",
    "# Count dimensions by performance tier\n",
    "excellent_dims = sum(1 for r in ast_per_dim_corr if r >= 0.7)\n",
    "good_dims = sum(1 for r in ast_per_dim_corr if 0.5 <= r < 0.7)\n",
    "moderate_dims = sum(1 for r in ast_per_dim_corr if 0.3 <= r < 0.5)\n",
    "poor_dims = sum(1 for r in ast_per_dim_corr if r < 0.3)\n",
    "\n",
    "print(f\"\\n📊 Performance Distribution:\")\n",
    "print(f\"   • Excellent (≥0.7): {excellent_dims}/19 dimensions ({excellent_dims/19*100:.1f}%)\")\n",
    "print(f\"   • Good (0.5-0.7): {good_dims}/19 dimensions ({good_dims/19*100:.1f}%)\")\n",
    "print(f\"   • Moderate (0.3-0.5): {moderate_dims}/19 dimensions ({moderate_dims/19*100:.1f}%)\")\n",
    "print(f\"   • Poor (<0.3): {poor_dims}/19 dimensions ({poor_dims/19*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n✅ All visualizations saved to Google Drive!\")\n",
    "print(f\"   • /content/drive/MyDrive/piano_transformer/evaluation_*.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 📝 Cell 6: Final Report and Conclusions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📝 Generating Final Evaluation Report\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generate comprehensive final report\n",
    "report = f\"\"\"\n",
    "# 🎹 Piano Perception Transformer - Final Evaluation Report\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  \n",
    "**Model:** 12-layer Audio Spectrogram Transformer with Regression Head  \n",
    "**Dataset:** PercePiano (19 perceptual dimensions)  \n",
    "**Evaluation:** Comprehensive comparison with baseline models\n",
    "\n",
    "## 📊 Executive Summary\n",
    "\n",
    "The Piano Perception Transformer achieved the following performance metrics:\n",
    "\n",
    "### 🎯 Key Performance Indicators\n",
    "- **Overall Pearson Correlation:** {evaluation_results['ast_metrics']['pearson_correlation']:.4f}\n",
    "- **Mean Per-Dimension Correlation:** {evaluation_results['ast_metrics']['mean_per_dim_correlation']:.4f} ± {evaluation_results['ast_metrics']['std_per_dim_correlation']:.4f}\n",
    "- **Root Mean Squared Error (RMSE):** {evaluation_results['ast_metrics']['rmse']:.4f}\n",
    "- **Mean Absolute Error (MAE):** {evaluation_results['ast_metrics']['mae']:.4f}\n",
    "- **R² Score:** {evaluation_results['ast_metrics']['r2_score']:.4f}\n",
    "\n",
    "### 🏆 Competitive Analysis\n",
    "- **Model Ranking:** {ast_rank}/{total_models} among all evaluated models\n",
    "- **Performance vs. Best Model:** {ast_metrics['pearson_correlation']/best_model['Pearson Corr']*100:.1f}%\n",
    "- **Best Baseline Beaten:** {'✅ Yes' if ast_rank == 1 else '❌ No'}\n",
    "\n",
    "## 📈 Detailed Performance Analysis\n",
    "\n",
    "### Per-Dimension Performance Distribution:\n",
    "- **Excellent (r ≥ 0.7):** {excellent_dims}/19 dimensions ({excellent_dims/19*100:.1f}%)\n",
    "- **Good (0.5 ≤ r < 0.7):** {good_dims}/19 dimensions ({good_dims/19*100:.1f}%)\n",
    "- **Moderate (0.3 ≤ r < 0.5):** {moderate_dims}/19 dimensions ({moderate_dims/19*100:.1f}%)\n",
    "- **Poor (r < 0.3):** {poor_dims}/19 dimensions ({poor_dims/19*100:.1f}%)\n",
    "\n",
    "### Best Performing Dimensions:\n",
    "\"\"\"\n",
    "\n",
    "# Add top 5 dimensions\n",
    "sorted_dims = sorted(enumerate(ast_per_dim_corr), key=lambda x: x[1], reverse=True)\n",
    "for i, (dim_idx, corr) in enumerate(sorted_dims[:5]):\n",
    "    report += f\"{i+1}. **{DIMENSION_NAMES[dim_idx]}:** {corr:.4f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "### Worst Performing Dimensions:\n",
    "\"\"\"\n",
    "\n",
    "# Add bottom 5 dimensions\n",
    "for i, (dim_idx, corr) in enumerate(sorted_dims[-5:]):\n",
    "    report += f\"{i+1}. **{DIMENSION_NAMES[dim_idx]}:** {corr:.4f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## 🏆 Baseline Comparison\n",
    "\n",
    "| Model | Pearson Corr | Mean Per-Dim Corr | RMSE | MAE | R² Score |\n",
    "|-------|--------------|-------------------|------|-----|----------|\n",
    "\"\"\"\n",
    "\n",
    "# Add comparison table\n",
    "for _, row in evaluation_results['comparison_df'].iterrows():\n",
    "    model_name = row['Model'].replace('🎹 ', '').replace('🏆 ', '')\n",
    "    report += f\"| {model_name} | {row['Pearson Corr']:.4f} | {row['Mean Per-Dim Corr']:.4f} | {row['RMSE']:.4f} | {row['MAE']:.4f} | {row['R² Score']:.4f} |\\n\"\n",
    "\n",
    "# Performance assessment\n",
    "if ast_metrics['pearson_correlation'] >= 0.7:\n",
    "    performance_assessment = \"🎉 **OUTSTANDING PERFORMANCE** - The model achieves excellent correlation (≥0.7) indicating strong predictive capability for perceptual dimensions.\"\n",
    "elif ast_metrics['pearson_correlation'] >= 0.5:\n",
    "    performance_assessment = \"✅ **GOOD PERFORMANCE** - The model demonstrates solid predictive capability with good correlation (≥0.5) across perceptual dimensions.\"\n",
    "elif ast_metrics['pearson_correlation'] >= 0.3:\n",
    "    performance_assessment = \"⚠️ **MODERATE PERFORMANCE** - The model shows moderate predictive capability but has room for significant improvement.\"\n",
    "else:\n",
    "    performance_assessment = \"❌ **POOR PERFORMANCE** - The model demonstrates limited predictive capability and requires substantial improvements.\"\n",
    "\n",
    "report += f\"\"\"\n",
    "## 🔍 Key Findings\n",
    "\n",
    "### Overall Assessment\n",
    "{performance_assessment}\n",
    "\n",
    "### Strengths\n",
    "- **Architecture:** The 12-layer Audio Spectrogram Transformer successfully processes mel-spectrograms for perceptual prediction\n",
    "- **Pre-training Benefits:** {'✅ Pre-training on MAESTRO likely contributed to feature learning' if os.path.exists('/content/drive/MyDrive/piano_transformer/checkpoints/ssast_pretraining/pretrained_for_finetuning.pkl') else '⚠️ No pre-training detected - may limit performance'}\n",
    "- **Best Dimensions:** Strong performance on {DIMENSION_NAMES[best_dim_idx]} (r = {ast_per_dim_corr[best_dim_idx]:.4f})\n",
    "- **Consistency:** {'Good' if evaluation_results['ast_metrics']['std_per_dim_correlation'] < 0.2 else 'Variable'} consistency across dimensions (std = {evaluation_results['ast_metrics']['std_per_dim_correlation']:.4f})\n",
    "\n",
    "### Areas for Improvement\n",
    "- **Challenging Dimensions:** {DIMENSION_NAMES[worst_dim_idx]} shows lowest performance (r = {ast_per_dim_corr[worst_dim_idx]:.4f})\n",
    "- **Model Complexity:** Consider architecture adjustments for better dimension-specific modeling\n",
    "- **Data Augmentation:** Additional data augmentation strategies may improve generalization\n",
    "- **Loss Function:** Experiment with dimension-weighted or adversarial losses\n",
    "\n",
    "## 📋 Technical Specifications\n",
    "\n",
    "### Model Architecture\n",
    "- **Backbone:** 12-layer Audio Spectrogram Transformer\n",
    "- **Parameters:** {sum(x.size for x in jax.tree.leaves(model_params)):,} total parameters\n",
    "- **Input:** 128×128 mel-spectrograms from MIDI synthesis\n",
    "- **Output:** 19 perceptual dimensions (normalized)\n",
    "- **Patch Size:** 16×16\n",
    "- **Embedding Dimension:** 768\n",
    "- **Attention Heads:** 12 per layer\n",
    "\n",
    "### Training Configuration\n",
    "- **Pre-training:** Self-supervised learning on MAESTRO dataset\n",
    "- **Fine-tuning:** Supervised learning on PercePiano dataset\n",
    "- **Loss Function:** MSE + correlation-based loss\n",
    "- **Optimization:** AdamW with cosine learning rate schedule\n",
    "- **Regularization:** Dropout (0.1) + stochastic depth (0.1)\n",
    "\n",
    "### Dataset Statistics\n",
    "- **Training Samples:** {len(train_dataset)}\n",
    "- **Validation Samples:** {len(val_dataset)}\n",
    "- **Test Samples:** {len(test_dataset)}\n",
    "- **Label Normalization:** StandardScaler applied\n",
    "\n",
    "## 🚀 Recommendations\n",
    "\n",
    "### For Production Deployment\n",
    "{'✅ **READY FOR DEPLOYMENT** - Model shows strong performance suitable for practical applications' if ast_metrics['pearson_correlation'] >= 0.6 else '⚠️ **NEEDS IMPROVEMENT** - Additional development recommended before deployment'}\n",
    "\n",
    "### For Further Research\n",
    "1. **Architecture Improvements:**\n",
    "   - Experiment with different patch sizes and attention mechanisms\n",
    "   - Consider hierarchical or multi-scale processing\n",
    "   - Investigate dimension-specific attention heads\n",
    "\n",
    "2. **Training Enhancements:**\n",
    "   - Implement curriculum learning or progressive training\n",
    "   - Explore advanced augmentation techniques\n",
    "   - Consider meta-learning approaches for few-shot adaptation\n",
    "\n",
    "3. **Evaluation Extensions:**\n",
    "   - Human evaluation studies for perceptual validation\n",
    "   - Cross-dataset generalization tests\n",
    "   - Real-time inference performance optimization\n",
    "\n",
    "## 📊 Supporting Materials\n",
    "\n",
    "All evaluation visualizations and detailed results have been saved to:\n",
    "- `evaluation_model_comparison.png` - Model performance comparison\n",
    "- `evaluation_per_dimension.png` - Per-dimension analysis\n",
    "- `evaluation_scatter_plots.png` - Prediction vs. target analysis\n",
    "- `evaluation_error_analysis.png` - Error distribution analysis\n",
    "- `evaluation_heatmap.png` - Performance heatmap\n",
    "\n",
    "---\n",
    "**Report Generated by Piano Perception Transformer Evaluation Pipeline**  \n",
    "**Framework:** JAX/Flax | **Hardware:** {jax.devices()[0].device_kind} | **Date:** {datetime.now().strftime('%Y-%m-%d')}\n",
    "\"\"\"\n",
    "\n",
    "# Save report to file\n",
    "report_path = '/content/drive/MyDrive/piano_transformer/Final_Evaluation_Report.md'\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "# Display key sections of the report\n",
    "print(\"🎹 PIANO PERCEPTION TRANSFORMER - FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n📊 PERFORMANCE METRICS:\")\n",
    "print(f\"   • Overall Correlation: {evaluation_results['ast_metrics']['pearson_correlation']:.4f}\")\n",
    "print(f\"   • Mean Per-Dimension: {evaluation_results['ast_metrics']['mean_per_dim_correlation']:.4f} ± {evaluation_results['ast_metrics']['std_per_dim_correlation']:.4f}\")\n",
    "print(f\"   • RMSE: {evaluation_results['ast_metrics']['rmse']:.4f}\")\n",
    "print(f\"   • R² Score: {evaluation_results['ast_metrics']['r2_score']:.4f}\")\n",
    "\n",
    "print(f\"\\n🏆 COMPETITIVE RANKING:\")\n",
    "print(f\"   • Rank: {ast_rank}/{total_models} models\")\n",
    "print(f\"   • vs Best: {ast_metrics['pearson_correlation']/best_model['Pearson Corr']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n📈 DIMENSION PERFORMANCE:\")\n",
    "print(f\"   • Excellent (≥0.7): {excellent_dims}/19 ({excellent_dims/19*100:.1f}%)\")\n",
    "print(f\"   • Good (0.5-0.7): {good_dims}/19 ({good_dims/19*100:.1f}%)\")\n",
    "print(f\"   • Needs Work (<0.5): {moderate_dims + poor_dims}/19 ({(moderate_dims + poor_dims)/19*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 ASSESSMENT: {performance_assessment.split(' - ')[0]}\")\n",
    "\n",
    "# Final WandB logging\n",
    "try:\n",
    "    wandb.log({\n",
    "        \"final_overall_correlation\": evaluation_results['ast_metrics']['pearson_correlation'],\n",
    "        \"final_model_rank\": ast_rank,\n",
    "        \"excellent_dimensions_count\": excellent_dims,\n",
    "        \"good_dimensions_count\": good_dims,\n",
    "        \"evaluation_complete\": True\n",
    "    })\n",
    "    \n",
    "    # Save evaluation artifacts\n",
    "    wandb.save('/content/drive/MyDrive/piano_transformer/evaluation_*.png')\n",
    "    wandb.save('/content/drive/MyDrive/piano_transformer/Final_Evaluation_Report.md')\n",
    "    \n",
    "    print(f\"\\n✅ Results logged to WandB and artifacts saved\")\n",
    "except:\n",
    "    print(f\"\\n⚠️ WandB logging failed - results saved locally\")\n",
    "\n",
    "print(f\"\\n📋 COMPLETE EVALUATION REPORT SAVED:\")\n",
    "print(f\"   📄 {report_path}\")\n",
    "print(f\"   📊 /content/drive/MyDrive/piano_transformer/evaluation_*.png\")\n",
    "\n",
    "print(f\"\\n🎉 COMPREHENSIVE EVALUATION COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"🎹 Piano Perception Transformer evaluation pipeline finished.\")\n",
    "\n",
    "# Cleanup\n",
    "try:\n",
    "    wandb.finish()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\n✨ Thank you for using the Piano Perception Transformer! ✨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 🎉 Evaluation Complete!\n",
    "\n",
    "**Final Results:**\n",
    "- 📊 **Performance Metrics**: Comprehensive correlation and error analysis\n",
    "- 🏆 **Baseline Comparisons**: Evaluated against 4 baseline models\n",
    "- 📈 **Visualizations**: 5 detailed performance plots saved\n",
    "- 📝 **Final Report**: Complete markdown report generated\n",
    "\n",
    "**Key Deliverables:**\n",
    "1. **Final Evaluation Report**: `Final_Evaluation_Report.md`\n",
    "2. **Performance Visualizations**: `evaluation_*.png` files\n",
    "3. **WandB Logging**: Comprehensive metrics and artifacts\n",
    "4. **Model Assessment**: Production readiness evaluation\n",
    "\n",
    "**Next Steps:**\n",
    "- Review the detailed report for insights and recommendations\n",
    "- Consider architectural improvements based on dimension analysis\n",
    "- Plan production deployment or further research iterations\n",
    "\n",
    "---\n",
    "\n",
    "**🎹 Piano Perception Transformer Evaluation Pipeline Complete! 🎹**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}